**Layers used**  

- Convolution layers: For the model to learn edges, gradients, patterns, parts of objects and objects  
- ReLu: The best activation function ever!   
- MaxPool: To select important features from image  
- Kernel used: 3 x 3 which is the standard kernel used extensively  
- Batch normalization: to normalize within its batch  
- DropOut layer: to force all the neurons to learn and avoid overfitting  
- Loss function: Cross entropy loss  
- Optimizer: Adam  
- log_softmax: Preferred activation function for multi class classification  
